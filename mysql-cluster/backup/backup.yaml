apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-backup-s3
  namespace: validation
spec:
  schedule: "41 3 * * *" # 03:41 AM; randomize to avoid conflicts
  timeZone: "Asia/Tehran"  
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          volumes:
            - name: backup
              emptyDir: {}
          initContainers:
            - name: dumping-mysql
              image: bitnami/mysql:8.0-debian-12
              imagePullPolicy: IfNotPresent
              terminationMessagePolicy: FallbackToLogsOnError
              env:
                - name: MYSQL_HOST
                  value: "mysql-master-svc"   
                - name: MYSQL_USER
                  value: "root"
                - name: MYSQL_ROOT_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: mysql-secret
                      key: ROOT_PASSWORD
              command: ["/bin/bash","-lc"]
              args:
                - |
                  set -exuo pipefail
                  trap 'echo "[dump][ERROR] exit code $? at line $LINENO"' ERR

                  echo "[dump] printing key env (redacted pwd)"
                  echo "  MYSQL_HOST=$MYSQL_HOST"
                  echo "  MYSQL_USER=$MYSQL_USER"

                  echo "[dump] resolving service dns"
                  getent hosts "$MYSQL_HOST" || true

                  echo "[dump] mysql ping"
                  /opt/bitnami/mysql/bin/mysqladmin -h"$MYSQL_HOST" -u"$MYSQL_USER" -p"$MYSQL_ROOT_PASSWORD" ping

                  TS=$(date +%F-%H%M)
                  OUT=/backup/mysql-$TS.all-dbs.sql.gz

                  echo "[dump] starting mysqldump of ALL databases → $OUT"
                  /opt/bitnami/mysql/bin/mysqldump \
                    -h"$MYSQL_HOST" -u"$MYSQL_USER" -p"$MYSQL_ROOT_PASSWORD" \
                    --all-databases \
                    --single-transaction --routines --events --triggers | gzip > "$OUT"

                  echo "[dump] verifying gzip integrity"
                  gzip -t "$OUT"

                  echo "[dump] listing backup dir"
                  ls -lah /backup

                  echo "[dump] checksum of artifact"
                  sha256sum "$OUT" || true
              volumeMounts:
                - name: backup
                  mountPath: /backup

          containers:
            - name: upload
              image: minio/mc:latest
              imagePullPolicy: IfNotPresent
              terminationMessagePolicy: FallbackToLogsOnError
              env:
                - name: RETENTION_DAYS
                  value: "14"        # keep 14 days
              envFrom:
                - secretRef: { name: s3-auth }
              command: ["/bin/sh","-c"]
              args:
                - |
                  set -exuo pipefail
                  echo "[upload] env check (redacted creds)"
                  echo "  S3_ENDPOINT=$S3_ENDPOINT"
                  echo "  S3_BUCKET=$S3_BUCKET"
                  echo "  RETENTION_DAYS=${RETENTION_DAYS:-14}"

                  echo "[upload] alias set"
                  mc alias set arvan-write "$S3_ENDPOINT" "$S3_ACCESS_KEY_ID_WRITE" "$S3_SECRET_ACCESS_KEY_WRITE" --insecure

                  echo "[upload] ensure bucket exists"
                  mc mb --ignore-existing "$S3_BUCKET" --insecure

                  echo "[upload] find dump file"
                  DUMP="$(ls -1 /backup/*.gz 2>/dev/null | head -n 1 || true)"
                  if [ -z "$DUMP" ]; then
                    echo "[upload][FATAL] no backup file found in /backup"; exit 1
                  fi
                  BASE="$(basename "$DUMP")"
                  echo "[upload] uploading $BASE → $S3_BUCKET/"
                  ls -lah "$DUMP"

                  mc cp "$DUMP" "arvan-write/$S3_BUCKET/$BASE" --insecure

                  echo "[upload] verifying remote object exists"
                  mc stat "arvan-write/$S3_BUCKET/$BASE" --insecure >/dev/null

                  echo "[upload] upload OK → $BASE"
                  echo "[upload] remote listing:"
                  mc ls "$S3_BUCKET" --insecure || true

                  echo "[upload] cleaning local temp"
                  rm -f /backup/*.gz || true

                  echo "[upload] retention: deleting objects older than ${RETENTION_DAYS:-14} days"
                  HOURS=$(( ${RETENTION_DAYS:-14} * 24 ))
                  mc rm --recursive --force --older-than "${HOURS}h" "$S3_BUCKET/" --insecure || true

                  echo "[upload] done."
              volumeMounts:
                - name: backup
                  mountPath: /backup
